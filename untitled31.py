# -*- coding: utf-8 -*-
"""Untitled31.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rapSZ7v1y-ZD3MUJ7IIfXtG9AkiCN_QO
"""

from google.colab import drive
drive.mount('/content/drive')

cd /content/drive/MyDrive/Autoencoder/

ls

# Download Dataset
#!wget http://images.cocodataset.org/zips/train2014.zip

# Extract Dataset
!mkdir data
#!unzip train2014.zip -d ./data >> /dev/null

# Commented out IPython magic to ensure Python compatibility.
from matplotlib import pyplot as plt
import numpy as np

import torch
import torchvision
from torch import nn
from torch.autograd import Variable
from torch.utils.data import DataLoader
from torchvision import transforms
import torchvision.datasets as dset
from torchvision.utils import save_image
import torchvision.utils as vutils
from torchsummary import summary
from IPython import display

# %matplotlib inline

# Configurations
dataroot = "./data"

image_size = 256
nb_channls=3
workers=4
num_epochs = 10
batch_size = 64
learning_rate = 1e-3
weight_decay = 1e-5

device = torch.device("cuda:0" if (torch.cuda.is_available()) else "cpu")

    
def to_img(x):
    x = 0.5 * (x + 1)
    x = x.clamp(0, 1)
    x = x.view(x.size(0), nb_channls, image_size, image_size)
    return x

device

from torch.utils.data.sampler import SubsetRandomSampler
# Load Dataset
dataset = dset.ImageFolder(root=dataroot,
                           transform=transforms.Compose([
                               transforms.Resize((image_size, image_size)),
                               transforms.ToTensor(),
                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
                           ]))


random_seed= 42
shuffle_dataset = True
validation_split = 0.2
dataset_size = len(dataset)
indices = list(range(dataset_size))
split = int(np.floor(validation_split * dataset_size))
if shuffle_dataset :
    np.random.seed(random_seed)
    np.random.shuffle(indices)
train_indices, val_indices = indices[split:], indices[:split]

train_sampler = SubsetRandomSampler(train_indices)
valid_sampler = SubsetRandomSampler(val_indices)

train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,
                                         num_workers=workers,sampler=train_sampler )

val_dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,
                                         num_workers=workers,sampler=valid_sampler )

# Preview some images in dataset
batch = next(iter(train_dataloader))
plt.figure(figsize=(8,8))
plt.axis("off")
btch = batch[0].to(device)[:64]
plt.imshow(np.transpose(vutils.make_grid(btch, padding=2, normalize=True).cpu(),(1,2,0)))

class autoencoder(nn.Module):
    def __init__(self):
        super(autoencoder, self).__init__()
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 16, kernel_size=5,stride=2),
            nn.Tanh(),
            nn.Conv2d(16,32,kernel_size=5,stride=2),
            nn.Tanh(),
            nn.Conv2d(32,64,kernel_size=5,stride=2),
            nn.Tanh(),
            nn.Conv2d(64,128,kernel_size=5,stride=2),
            nn.Tanh(),
            nn.Conv2d(128,256,kernel_size=5,stride=2),
            nn.Tanh(),

            ) 
        
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(256,128,kernel_size=5,stride=2),
            nn.Tanh(),
            nn.ConvTranspose2d(128,64,kernel_size=5,stride=2),
            nn.Tanh(),
            nn.ConvTranspose2d(64,32,kernel_size=5,stride=2),
            nn.Tanh(),
            nn.ConvTranspose2d(32,16,kernel_size=5,stride=2,output_padding=1),
            nn.Tanh(),
            nn.ConvTranspose2d(16,3,kernel_size=5,stride=2,output_padding=1),
            nn.Tanh(),
            )    
    
    def forward(self,x):
        x = self.encoder(x)
        x = self.decoder(x)
        return x

# Compile Network
model = autoencoder().to(device)

optimizer = torch.optim.Adam(
    model.parameters(), lr=learning_rate, weight_decay=weight_decay)
summary(model, (nb_channls, image_size, image_size))

# Define Loss function
def mse_loss(input, target):
    r = input[:,0:1,:,:] - target[:,0:1,:,:]
    g = (input[:,1:2,:,:] - target[:,1:2,:,:])
    b = input[:,2:3,:,:] - target[:,2:3,:,:]
    
    r = torch.mean(r**2)
    g = torch.mean(g**2)
    b = torch.mean(b**2)
    
    mean = (r + g + b)/3
   
    return mean, r,g,b

# Training Loop. Results will appear every 50th iteration.
itr = 0
for epoch in range(num_epochs):
    for data in train_dataloader:
        img, _ = data        
        img = Variable(img).to(device)

        # Forward
        output = model(img)
        loss,r_loss,g_loss,b_loss = mse_loss(output, img)
        
        # Backprop
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
       
        if itr % 50 == 0:
            # Log
            print('iter [{}], loss:{:.4f} {:.4f} {:.4f} {:.4f}'
              .format(itr, loss.data.item(), r_loss.data.item(), g_loss.data.item(), b_loss.data.item()))
            pic = to_img(output.to("cpu").data)
        
          
#             fig = plt.figure(  figsize=(16, 16))
#             btch = pic.to(device)[:64]
#             ax = plt.imshow(np.transpose(vutils.make_grid(btch, padding=2, normalize=True).cpu(),(1,2,0)))
#             ax.axes.get_xaxis().set_visible(False)
#             ax.axes.get_yaxis().set_visible(False)
#             plt.show(fig)
            
            
            with torch.no_grad():
              val_data = next(iter(val_dataloader))
              val_img, _ = val_data
              val_img = Variable(val_img).to(device)
              val_output = model(val_img)
              val_loss, val_r_loss, val_g_loss, val_b_loss = mse_loss(val_output, val_img)
              
              print('iter [{}], val loss:{:.4f} {:.4f} {:.4f} {:.4f}'
                .format(itr, val_loss.data.item(), val_r_loss.data.item(), val_g_loss.data.item(), val_b_loss.data.item()))
              
              val_pic = to_img(val_output.to("cpu").data)
              val_pic_orig = val_data[0]
              btch_list = []
              for i in range(val_pic.shape[0]):
                btch_list.append(val_pic_orig[i:i+1,:,:,:])
                btch_list.append(transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]) (val_pic[i,:,:,:]).unsqueeze(0))
              
              btch = torch.cat(btch_list)
              
              fig = plt.figure(  figsize=(16, 16))
              btch = btch.to(device)[:64]
              ax = plt.imshow(np.transpose(vutils.make_grid(btch, padding=2, normalize=True).cpu(),(1,2,0)))
              ax.axes.get_xaxis().set_visible(False)
              ax.axes.get_yaxis().set_visible(False)
              plt.show(fig)
        
            
        itr += 1

    print('epoch [{}/{}], loss:{:.4f}'
          .format(epoch + 1, num_epochs, loss.data.item()))

# Save Weights
torch.save(model.state_dict(), './coco_color_3-128-128_128_autoencoder_statedict.pth')

# Download Weights
from google.colab import files
files.download('./coco_color_3-128-128_128_autoencoder_statedict.pth')